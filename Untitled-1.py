import requests
from bs4 import BeautifulSoup
import re

def fetch_search_results(url, query):
    """Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ GET Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ 3 Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ø¨ØµÙŠØºØ© JSON."""
    try:
        response = requests.get(url, params={"q": query, "format": "json"}, timeout=10)
        response.raise_for_status()
        data = response.json().get("results", [])
        return [{"title": item.get("title", "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"), "url": item.get("url", "")} for item in data[:3]]  # Ø¬Ù„Ø¨ Ø£ÙˆÙ„ 3 Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return []

def highlight_match(paragraph, query):
    """ØªÙ„ÙˆÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨ÙŠÙ† Ø§Ù„ÙÙ‚Ø±Ø© ÙˆÙ†Øµ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø­Ù…Ø±."""
    words = query.split()  # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
    pattern = re.compile(r'\b(' + '|'.join(re.escape(word) for word in words) + r')\b', re.IGNORECASE)  # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„ ÙƒÙ„Ù…Ø©
    highlighted = pattern.sub(r'\033[91m\1\033[0m', paragraph)  # ØªÙ„ÙˆÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø§Ù„Ø£Ø­Ù…Ø±
    return highlighted

def extract_matching_paragraph(url, query):
    """Ø¬Ù„Ø¨ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ù‹Ø§ Ù…Ø¹ Ù†Øµ Ø§Ù„Ø¨Ø­Ø« Ù…Ù† ØµÙØ­Ø© Ø§Ù„ÙˆÙŠØ¨."""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")  # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ø¹Ø¯Ø¯ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
        best_paragraph = None
        max_matches = 0

        for p in paragraphs:
            text = p.get_text(strip=True)
            matches = sum(1 for word in query.split() if word.lower() in text.lower())  # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            
            if matches > max_matches:
                max_matches = matches
                best_paragraph = text
        
        if best_paragraph:
            return highlight_match(best_paragraph, query)
        return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‚Ø±Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø©"
    
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {url}: {e}")
        return "âŒ ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"

# ğŸ”¥ ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒÙˆØ¯
search_url = "http://localhost:8080/search"  # ØºÙŠÙ‘Ø±Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù€ API Ø§Ù„ØµØ­ÙŠØ­
query = "The general problem of simulating intelligence has been broken into subproblems"

# Ø¬Ù„Ø¨ Ø£ÙˆÙ„ 3 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
search_results = fetch_search_results(search_url, query)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ·Ø§Ø¨Ù‚Ù‹Ø§ Ù…Ù† ÙƒÙ„ Ø±Ø§Ø¨Ø·
for entry in search_results:
    matched_paragraph = extract_matching_paragraph(entry["url"], query)
    print(f"ğŸ“Œ {entry['title']}\nğŸ”— {entry['url']}\nğŸ“ {matched_paragraph}\n{'-'*80}")
